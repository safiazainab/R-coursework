### DATA SCIENCE MODELLING - WEEK 8: TREE-BASED METHODS
#
# R examples from Chapter 8 of the book
# "An Introduction to Statistical Learning with Applications in R"
# (some explanations and modifications added by Andras Voros)
#
###

### INSTRUCTIONS: Try out and aim to understand the commands in this script.
#                 This will help you to solve the practical exercises and
#                 be able to use the methods on your own.
#                 Extensive notes are provided to help your learning.
#                 If you find something unclear, feel free to ask about it
#                 in the week's forum or in the live lecture session.

### CONTENTS
#   0. Install/load packages
#   1. The datasets
#   2. Classification Trees
#   3. Regression Trees
#   4. Bagging and Random Forests
#   5. Boosting



### Chapter 8 Lab: Tree-based Methods


## 0. Install and load required packages

# load the package of the book
# if you don't have it yet, run the following line to install it
# install.packages("ISLR") # remove hashtag from beginning of the line to run it
library(ISLR)

# install and load the R package tree in which various tree-based methods are implemented
install.packages("tree") # install the package if you have not done so earlier
library(tree) # load it

# install and load the R package MASS, which contains a number of useful datasets
# we will use the dataset called "Boston" from this package for the regression tree examples
install.packages("MASS") # install the package if you have not done so earlier
library(MASS) # load it

# install and load the R package randomForest that implements bagging and random forests
install.packages("randomForest") # install the package if you have not done so earlier
library(randomForest) # load it

# finally, install and load the R package gbm that implements various boosting models
install.packages("gbm") # install the package if you have not done so earlier
library(gbm) # load it

# now you can start working on the examples!


## 1. The datasets

# we use two datasets in the examples

# a) the "Carseats" data from the ISLR package is a simulated dataset containing sales records
# from 400 stores that sell child car seats, along with a number of predictors; we will recode
# the number of seats sold into a binary variable and apply classification trees to the data

# let's inspect the data object a little bit
?Carseats # help page explaining what is exactly in the dataset
class(Carseats) # it's a data frame object as usual
dim(Carseats) # 400 observations and 11 variables
names(Carseats) # variable names
head(Carseats) # look at the first few rows

# b) the "Boston" data from the MASS package is a dataset containing residential property values
# in 506 suburbs of Boston, along with area-level predictors for value; we will use this dataset
# to fit and improve regression trees

# we can quickly check how these data are stored as well
?Boston # help page explaining what is exactly in the dataset
class(Boston) # it's a data frame object as usual
dim(Boston) # 506 observations and 14 variables
names(Boston) # variable names ('medv' will be the outcome: median value of properties
              #                 in each suburb, in $1,000s)
head(Boston) # look at the first few rows

# we are ready to start the analyses!


## 2. Fitting Classification Trees

# we first fit a classiciation tree explaining whether a store has "high" or "low" child
# car seat sales numbers (based on an arbitrary threshold)

# let's attach the Carseats dataset so that we can more easily refer to its variables 
attach(Carseats)

# we define the outcome: high sales ("Yes") if sales>8, low sales ("No") if sales<=8
High <- factor(ifelse(Sales<=8,"No","Yes"))
# we add the new variable as the last column of the dataset
Carseats <- data.frame(Carseats, High)

# now, we fit a decision tree explaning the High variable with all other variables in the
# Carseats dataset, except for Sales (for obvious reasons)
tree.carseats <- tree(High~.-Sales, Carseats) # use tree function from the tree package
summary(tree.carseats) # misclassification error is 9% (on training data)
plot(tree.carseats)    # plot tree
text(tree.carseats)    # add labels (you will have to zoom in or print this in a file with
                       #             larger figure area if you want the labels to be spaced nicely)
tree.carseats          # more information about the tree

# based on the plot shelving location and price seem to be the most important predictors of sales
# and we saw that the model has a training error of 9%

# we now estimate the test error - we use the validation set approach

# randomly select half the sample to be in the training set
train <- sample(1:nrow(Carseats), 200)
# test set is the data without the training observations
Carseats.test <- Carseats[-train,]
# also drop training observations from the test outcome vector
High.test <- High[-train]
# fit tree to training data
tree.carseats <- tree(High~.-Sales, Carseats, subset=train)
# predict for test cases using the fitted model
tree.pred <- predict(tree.carseats, Carseats.test, type="class")
# use a crosstable to check how the predictions match the test outcomes
test.table <- table(tree.pred, High.test)
# note: the result will be different between computers and runs, because we sampled the
#       training set randomly
# the % of cases classified correctly:
(test.table[1,1] + test.table[2,2]) / sum(test.table)

# now we carry out cost complexity pruning
# we use cross-validation to determine the optimal tuning parameter
# this is quite simple with the cv.tree() function
cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass) # last argument specifies that we want
                                                          # to use classificaiton error rate to
                                                          # guide the cv and pruning process
# what is in the result object?
names(cv.carseats) # size is no. of terminal nodes in the trees
                   # k is the tuning parameter (alpha)
# the results
cv.carseats # dev is now actually the cross-validation error rate
# find the size of the tree with the lowerst error
best.size <- cv.carseats$size[which(cv.carseats$dev==min(cv.carseats$dev))]

# plot the error rate as a function of size and k
par(mfrow=c(1,2)) # pring two plots side by side in one figure
plot(cv.carseats$size, cv.carseats$dev, type="b") # plot size by error
plot(cv.carseats$k, cv.carseats$dev, type="b")    # plot tuning parameter k by error
par(mfrow=c(1,1)) # reset to one plot per figure

# now, let's prune the tree to the size with the lowest error...
prune.carseats <- prune.misclass(tree.carseats, best=best.size)
# ... and plot the resulting tree
plot(prune.carseats)
text(prune.carseats)

# check how well the pruned tree performs on the test data
tree.pred <- predict(prune.carseats, Carseats.test, type="class")
test.table2 <- table(tree.pred, High.test)
# the % of cases classified correctly:
(test.table2[1,1] + test.table2[2,2]) / sum(test.table2)


## 3. Fitting Regression Trees

# we now fit a regression tree explaining whether the mean value of properties in
# the suburbs of Boston

# first, we sample a training set and fit the tree to this
train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston <- tree(medv~., Boston, subset=train)
# the result
summary(tree.boston) # note that not all of the variables were used in the tree
                     # deviance is the sum of squared errors in this case
# plotting the tree
plot(tree.boston)
text(tree.boston)

# let's prune the tree with the same approach as earlier
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type='b')
# identify the size of the best fitting tree
best.boston <- cv.boston$size[cv.boston$dev==min(cv.boston$dev)]

# prune tree to optimal size (note: it may be the same as the original tree)
prune.boston <- prune.tree(tree.boston, best=best.boston)
# plot the pruned tree
plot(prune.boston)
text(prune.boston)

# make predictions on the test set, using the pruned tree
yhat <- predict(prune.boston, newdata=Boston[-train,]) # predicted values from tree
boston.test <- Boston[-train,"medv"]                   # actual observed values of median price
# plot predicted against observed
plot(yhat, boston.test)
abline(0,1)
# test MSE:
mean((yhat-boston.test)^2)


## 4. Bagging and Random Forests

# here we fit random forests on the Boston data
# remember that bagging is random forests with all p predictors considered at each split

# let's try baggin first on the training set from above
# mtry tells the function to include all 13 variables in each split
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=13, importance=TRUE)
bag.boston # overview of the reuslt
# how well does this perform on the test set? (same steps as before)
yhat.bag <- predict(bag.boston, newdata=Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
# the MSE should be a lot smaller than what we got from a single pruned tree above

# we can also set the number of trees to grow by the ntree argument
bag.boston <- randomForest(medv~., data=Boston, subset=train, mtry=13, ntree=25)
yhat.bag <- predict(bag.boston, newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2) # a different test MSE

# now an actual random forest, with mtry<13
rf.boston <- randomForest(medv~., data=Boston, subset=train, mtry=6, importance=TRUE)
yhat.rf <- predict(rf.boston, newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
# the test MSE in this case should be somewhat better than from bagging above

# we can check vairable importance
importance(rf.boston) # two measures: first is mean increase in MSE when the variable is excluded
                      #               second is training RSS
# plot the two importance measures per variable
varImpPlot(rf.boston)


## 5. Boosting

# finally, let's perform boosting on the Boston data
boost.boston <- gbm(medv~.,                  # explain median value with the rest of the variables
                    data=Boston[train,],     # using the training observations of the dataset
                    distribution="gaussian", # use this for regressions, "bernoulli" for classifications
                    n.trees=5000,            # no. trees to be fit
                    interaction.depth=4)     # maximum depth of each tree
summary(boost.boston) # relative influence of vairables is plotted

# we can also plot the marginal effects of the variables (such as rm and lstat)
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
par(mfrow=c(1,1))

# predict the test outcomes with the boosted model
yhat.boost <- predict(boost.boston, newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost-boston.test)^2)

# fit and predict with a different shrinkage parameter
boost.boston <- gbm(medv~.,
                    data=Boston[train,],
                    distribution="gaussian",
                    n.trees=5000,
                    interaction.depth=4,
                    shrinkage=0.2)
yhat.boost <- predict(boost.boston, newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost-boston.test)^2)
# test MSE may be slightly lower or higher than with the default shrinkage parameter


### END OF SCRIPT.
